{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88d1868c-bfd2-4ef4-b359-36ad85f1923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Fichier : consolide/adjudication_CARRAUD_petite-Jeanne_TesseractFra-PNG_spaCy3.7.2-lg_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 376\n",
      "   🔹 Nombre d'annotations Quaero : 5\n",
      "📂 Fichier : consolide/adjudication_CARRAUD_petite-Jeanne_REF_spaCy3.7.2-lg_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 384\n",
      "   🔹 Nombre d'annotations Quaero : 12\n",
      "📂 Fichier : consolide/adjudication_CARRAUD_petite-Jeanne_Kraken-base_spaCy3.7.2-lg 1_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 374\n",
      "   🔹 Nombre d'annotations Quaero : 8\n",
      "📂 Fichier : consolide/adjudication_DAUDET_petit-chose_Kraken-base_spaCy3.7.2-lg_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 260\n",
      "   🔹 Nombre d'annotations Quaero : 38\n",
      "📂 Fichier : consolide/adjudication_DAUDET_petit-chose_REF_spaCy3.7.2-lg_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 259\n",
      "   🔹 Nombre d'annotations Quaero : 33\n",
      "📂 Fichier : consolide/adjudication_DAUDET_petit-chose_TesseractFra-PNG_spaCy3.7.2-lg_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 237\n",
      "   🔹 Nombre d'annotations Quaero : 34\n",
      "✅ Tous les fichiers ont été combinés et sauvegardés sous 'combined_data_output.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Dossier contenant les fichiers CSV\n",
    "folder_path = \"consolide\"  #\n",
    "\n",
    "# Fonction pour créer un format JSON à partir des données CSV\n",
    "def convert_csv_to_json(file_path):\n",
    "    try:\n",
    "        # Charger le fichier CSV\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "        # Reconstituer le texte complet en s'assurant que tous les tokens sont des chaînes\n",
    "        tokens = df[\"Token\"].fillna(\"\").astype(str).tolist()  # Remplacer les NaN par une chaîne vide et convertir en str\n",
    "        full_text = \" \".join(tokens)\n",
    "\n",
    "        # Statistiques pour le fichier\n",
    "        num_tokens = len(tokens)\n",
    "        num_annotations = 0\n",
    "        num_quaero_annotations = 0\n",
    "\n",
    "        # Préparer les annotations\n",
    "        annotations = []\n",
    "        for i, row in df.iterrows():\n",
    "            token = str(row[\"Token\"])  # Convertir en chaîne de caractères\n",
    "            label = row[\"Label_Maj\"]\n",
    "            quaero_label = row[\"Quaero_Maj\"] if pd.notna(row[\"Quaero_Maj\"]) else None\n",
    "\n",
    "            # Si l'annotation est présente\n",
    "            if pd.notna(label):\n",
    "                # Calculer la position du token dans le texte complet\n",
    "                start = full_text.find(token)\n",
    "                end = start + len(token)\n",
    "\n",
    "                # Ajouter l'annotation au format requis\n",
    "                annotation = {\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"tag_id\": str(uuid.uuid4()),  # Tag ID unique\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"example_id\": str(uuid.uuid4()),\n",
    "                    \"tag_name\": label,\n",
    "                    \"value\": token,\n",
    "                    \"correct\": None,\n",
    "                    \"human_annotations\": [],\n",
    "                    \"model_annotations\": []\n",
    "                }\n",
    "\n",
    "                # Si c'est une entité de type \"LOC\", vérifier si Quaero est aussi disponible\n",
    "                if label == \"LOC\" and quaero_label:\n",
    "                    annotation[\"tag_name\"] = f\"LOC, {quaero_label}\"  # Ajouter Quaero à l'annotation\n",
    "                    num_quaero_annotations += 1\n",
    "\n",
    "                annotations.append(annotation)\n",
    "                num_annotations += 1\n",
    "\n",
    "        # Préparer l'exemple\n",
    "        example = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"content\": full_text,\n",
    "            \"metadata\": {},\n",
    "            \"annotations\": annotations\n",
    "        }\n",
    "\n",
    "        # Suivi des statistiques pour l'affichage\n",
    "        print(f\"📂 Fichier : {file_path}\")\n",
    "        print(f\"   🔹 Nombre de tokens : {num_tokens}\")\n",
    "        print(f\"   🔹 Nombre d'annotations : {num_annotations}\")\n",
    "        print(f\"   🔹 Nombre d'annotations Quaero : {num_quaero_annotations}\")\n",
    "        \n",
    "        # Retourner les données au format JSON\n",
    "        return {\"examples\": [example]}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur avec {file_path} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Liste pour accumuler les données JSON de tous les fichiers\n",
    "all_data = {\"examples\": []}\n",
    "\n",
    "# Parcourir tous les fichiers du dossier\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".csv\"):  # Vérifie que c'est un fichier CSV\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Convertir le fichier CSV en format JSON\n",
    "        json_data = convert_csv_to_json(file_path)\n",
    "        \n",
    "        if json_data:\n",
    "            # Ajouter les données au fichier JSON principal\n",
    "            all_data[\"examples\"].extend(json_data[\"examples\"])\n",
    "\n",
    "# Sauvegarder tous les fichiers JSON dans un fichier de sortie unique\n",
    "output_file_path = \"combined_data_output.json\"\n",
    "with open(output_file_path, 'w') as json_file:\n",
    "    json.dump(all_data, json_file, indent=4)\n",
    "\n",
    "print(f\"✅ Tous les fichiers ont été combinés et sauvegardés sous '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eba1af25-d1d0-41b0-8dc5-9c3012a0a967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Fichier : consolide/adjudication_CARRAUD_petite-Jeanne_TesseractFra-PNG_spaCy3.7.2-lg_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 376\n",
      "   🔹 Nombre d'annotations Quaero : 5\n",
      "📂 Fichier : consolide/adjudication_CARRAUD_petite-Jeanne_REF_spaCy3.7.2-lg_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 384\n",
      "   🔹 Nombre d'annotations Quaero : 12\n",
      "📂 Fichier : consolide/adjudication_CARRAUD_petite-Jeanne_Kraken-base_spaCy3.7.2-lg 1_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 374\n",
      "   🔹 Nombre d'annotations Quaero : 8\n",
      "📂 Fichier : consolide/adjudication_DAUDET_petit-chose_Kraken-base_spaCy3.7.2-lg_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 260\n",
      "   🔹 Nombre d'annotations Quaero : 38\n",
      "📂 Fichier : consolide/adjudication_DAUDET_petit-chose_REF_spaCy3.7.2-lg_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 259\n",
      "   🔹 Nombre d'annotations Quaero : 33\n",
      "📂 Fichier : consolide/adjudication_DAUDET_petit-chose_TesseractFra-PNG_spaCy3.7.2-lg_consolide.csv\n",
      "   🔹 Nombre de tokens : 6500\n",
      "   🔹 Nombre d'annotations : 237\n",
      "   🔹 Nombre d'annotations Quaero : 34\n",
      "✅ Tous les fichiers ont été combinés et sauvegardés sous 'combined_data_output.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Dossier contenant les fichiers CSV\n",
    "folder_path = \"consolide\"  \n",
    "\n",
    "# Fonction pour créer un format JSON à partir des données CSV\n",
    "def convert_csv_to_json(file_path):\n",
    "    try:\n",
    "        # Charger le fichier CSV\n",
    "       # df = pd.read_csv(file_path, sep=';')\n",
    "        df = pd.read_csv(file_path, sep=';', encoding=\"utf-8\")\n",
    "\n",
    "        # Reconstituer le texte complet en s'assurant que tous les tokens sont des chaînes\n",
    "        tokens = df[\"Token\"].fillna(\"\").astype(str).tolist()  # Remplacer les NaN par une chaîne vide et convertir en str\n",
    "       # full_text = \" \".join(tokens)\n",
    "        #full_text = \" \".join(df[\"Token\"].dropna().astype(str)).strip()\n",
    "        # Reconstituer le texte complet sans espaces superflus ni sauts de ligne\n",
    "        full_text = \" \".join(df[\"Token\"].dropna().astype(str)).replace(\"\\n\", \"\").strip()\n",
    "\n",
    "        # Statistiques pour le fichier\n",
    "        num_tokens = len(tokens)\n",
    "        num_annotations = 0\n",
    "        num_quaero_annotations = 0\n",
    "\n",
    "        # Préparer les annotations\n",
    "        annotations = []\n",
    "        for i, row in df.iterrows():\n",
    "            token = str(row[\"Token\"])  # Convertir en chaîne de caractères\n",
    "            label = row[\"Label_Maj\"]\n",
    "            quaero_label = row[\"Quaero_Maj\"] if pd.notna(row[\"Quaero_Maj\"]) else None\n",
    "\n",
    "            # Si l'annotation est présente\n",
    "            if pd.notna(label):\n",
    "                # Calculer la position du token dans le texte complet\n",
    "                start = full_text.find(token)\n",
    "                end = start + len(token)\n",
    "\n",
    "                # Ajouter l'annotation au format requis\n",
    "                annotation = {\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"tag_id\": str(uuid.uuid4()),  # Tag ID unique\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                    \"example_id\": str(uuid.uuid4()),\n",
    "                    \"tag_name\": label,\n",
    "                    \"value\": token,\n",
    "                    \"correct\": None,\n",
    "                    \"human_annotations\": [],\n",
    "                    \"model_annotations\": []\n",
    "                }\n",
    "\n",
    "                # Si c'est une entité de type \"LOC\", vérifier si Quaero est aussi disponible\n",
    "                if label == \"LOC\" and quaero_label:\n",
    "                    annotation[\"tag_name\"] = f\"LOC, {quaero_label}\"  # Ajouter Quaero à l'annotation\n",
    "                    num_quaero_annotations += 1\n",
    "\n",
    "                annotations.append(annotation)\n",
    "                num_annotations += 1\n",
    "\n",
    "        # Préparer l'exemple\n",
    "        example = {\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"content\": full_text,\n",
    "            \"metadata\": {},\n",
    "            \"annotations\": annotations\n",
    "        }\n",
    "\n",
    "        # Suivi des statistiques pour l'affichage\n",
    "        print(f\"📂 Fichier : {file_path}\")\n",
    "        print(f\"   🔹 Nombre de tokens : {num_tokens}\")\n",
    "        print(f\"   🔹 Nombre d'annotations : {num_annotations}\")\n",
    "        print(f\"   🔹 Nombre d'annotations Quaero : {num_quaero_annotations}\")\n",
    "        \n",
    "        # Retourner les données au format JSON\n",
    "        return {\"examples\": [example]}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur avec {file_path} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Liste pour accumuler les données JSON de tous les fichiers\n",
    "all_data = {\"examples\": []}\n",
    "\n",
    "# Parcourir tous les fichiers du dossier\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".csv\"):  # Vérifie que c'est un fichier CSV\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Convertir le fichier CSV en format JSON\n",
    "        json_data = convert_csv_to_json(file_path)\n",
    "        \n",
    "        if json_data:\n",
    "            # Ajouter les données au fichier JSON principal\n",
    "            all_data[\"examples\"].extend(json_data[\"examples\"])\n",
    "\n",
    "# Sauvegarder tous les fichiers JSON dans un fichier de sortie unique\n",
    "output_file_path = \"combined_data_output.json\"\n",
    "with open(output_file_path, 'w', encoding=\"utf-8\") as json_file:\n",
    "    json.dump(all_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Tous les fichiers ont été combinés et sauvegardés sous '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72561ee9-9bf3-42be-8f0b-001aad9dfe6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
